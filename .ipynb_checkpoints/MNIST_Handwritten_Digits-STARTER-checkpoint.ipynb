{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the MNIST dataset.\n",
    "\n",
    "Some of the benchmark results on MNIST include can be found [on Yann LeCun's page](https://webcache.googleusercontent.com/search?q=cache:stAVPik6onEJ:yann.lecun.com/exdb/mnist) and include:\n",
    "\n",
    "88% [Lecun et al., 1998](https://hal.science/hal-03926082/document)\n",
    "\n",
    "95.3% [Lecun et al., 1998](https://hal.science/hal-03926082v1/document)\n",
    "\n",
    "99.65% [Ciresan et al., 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf)\n",
    "\n",
    "\n",
    "MNIST is a great dataset for sanity checking your models, since the accuracy levels achieved by large convolutional neural networks and small linear models are both quite high. This makes it important to be familiar with the data.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the PATH to include the user installation directory. \n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/root/.local/bin\"\n",
    "\n",
    "# Restart the Kernel before you move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: Restart the Kernel before you move on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list if you intend to .\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "MNIST is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `MNIST` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/vision/stable/datasets.html#mnist)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Jupyter\n",
    "!pip install --upgrade jupyter\n",
    "\n",
    "# Update ipywidgets\n",
    "!pip install --upgrade ipywidgets\n",
    "\n",
    "# Enable ipywidgets extension\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "\n",
    "# Install widgetsnbextension\n",
    "!pip install widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images to have mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "# Create training set and define training dataloader\n",
    "# Load the MNIST training dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "# Create DataLoader for the training dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "val_loader = DataLoader(train_dataset, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "# Create test set and define test dataloader\n",
    "# Load the MNIST test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "# Create DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify your preprocessing\n",
    "\n",
    "In your own words, why did you choose the transforms you chose? If you didn't use any preprocessing steps, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It was used to convert image data into Tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a function for showing 5 images from a dataloader – DO NOT CHANGE THE CONTENTS! ##\n",
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(int(labels[i].detach()))\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        plt.imshow(image.T.squeeze().T)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "print(\"No. of MNIST train data examples: {}\".format(len(train_dataset)))\n",
    "print(\"No. of MNIST test data examples: {}\".format(len(test_dataset)))\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "print(\"No. of images: {}\".format(images.shape))\n",
    "print(\"No. of labels: {}\".format(labels.shape))\n",
    "show5(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset.\n",
    "Use any architecture you like. \n",
    "\n",
    "*Note*: If you did not flatten your tensors in your transforms or as part of your preprocessing and you are using only `Linear` layers, make sure to use the `Flatten` layer in your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 64)       # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(64, 10)        # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)            # Flatten the image\n",
    "        x = F.relu(self.fc1(x))        # Apply ReLU activation\n",
    "        x = F.relu(self.fc2(x))        # Apply ReLU activation\n",
    "        x = self.fc3(x)                    # Output layer\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss **during** each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish lists for loss history\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "num_epochs = 5  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()  # Set the network to training mode\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()        # Zero the parameter gradients\n",
    "        outputs = net(inputs)        # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()              # Backward pass\n",
    "        optimizer.step()             # Optimize the weights\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)  # Calculate average training loss\n",
    "    train_accuracy = train_correct / len(train_loader.dataset)  # Calculate training accuracy\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} training accuracy: {train_accuracy * 100:.2f}% training loss: {train_loss:.5f}')\n",
    "\n",
    "    # Validation step\n",
    "    net.eval()  # Set the network to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)  # Calculate average validation loss\n",
    "    val_accuracy = val_correct / len(val_loader.dataset)  # Calculate validation accuracy\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} validation accuracy: {val_accuracy * 100:.2f}% validation loss: {val_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss history\n",
    "plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 90%, great work, but see if you can push a bit further! \n",
    "If your accuracy is under 90%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()  # Set the network to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            outputs = net(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)  # Calculate average test loss\n",
    "    test_accuracy = test_correct / len(test_loader.dataset)  # Calculate test accuracy\n",
    "    print(f'Test accuracy: {test_accuracy * 100:.2f}% test loss: {test_loss:.5f}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving your model\n",
    "\n",
    "Once your model is done training, try tweaking your hyperparameters and training again below to improve your accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a modified neural network with dropout\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 64)       # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(64, 10)        # Output layer\n",
    "        self.dropout = nn.Dropout(0.2)      # Dropout layer with 0.2 probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)             # Flatten the image\n",
    "        x = F.relu(self.fc1(x))             # Apply ReLU activation\n",
    "        x = self.dropout(x)                 # Apply dropout\n",
    "        x = F.relu(self.fc2(x))             # Apply ReLU activation\n",
    "        x = self.dropout(x)                 # Apply dropout\n",
    "        x = self.fc3(x)                     # Output layer\n",
    "        return x\n",
    "\n",
    "# Create the modified neural network and move it to the device\n",
    "net2 = Net2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish lists for loss history\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "num_epochs = 5  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net2.train()  # Set the network to training mode\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()        # Zero the parameter gradients\n",
    "        outputs = net2(inputs)        # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()              # Backward pass\n",
    "        optimizer.step()             # Optimize the weights\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)  # Calculate average training loss\n",
    "    train_accuracy = train_correct / len(train_loader.dataset)  # Calculate training accuracy\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} training accuracy: {train_accuracy * 100:.2f}% training loss: {train_loss:.5f}')\n",
    "\n",
    "    # Validation step\n",
    "    net2.eval()  # Set the network to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            \n",
    "            outputs = net2(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)  # Calculate average validation loss\n",
    "    val_accuracy = val_correct / len(val_loader.dataset)  # Calculate validation accuracy\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} validation accuracy: {val_accuracy * 100:.2f}% validation loss: {val_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss history\n",
    "plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2():\n",
    "    net2.eval()  # Set the network to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            outputs = net2(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)  # Calculate average test loss\n",
    "    test_accuracy = test_correct / len(test_loader.dataset)  # Calculate test accuracy\n",
    "    print(f'Test accuracy: {test_accuracy * 100:.2f}% test loss: {test_loss:.5f}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    test2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Net, \"originalmodel.pth\")\n",
    "torch.save(Net2, \"improvedmodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
